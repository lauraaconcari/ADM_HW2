{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOKIbjZCVdxbGW3FWnUCxup",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lauraaconcari/ADM_HW2/blob/main/RQ7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ENt03cf22MOX",
        "outputId": "97989263-59a7-4f3f-fcc9-ec94bd60cdab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-11-02 10:30:36--  https://sapienza2023adm.s3.eu-south-1.amazonaws.com/lighter_books.json\n",
            "Resolving sapienza2023adm.s3.eu-south-1.amazonaws.com (sapienza2023adm.s3.eu-south-1.amazonaws.com)... 52.95.153.12, 52.95.152.46\n",
            "Connecting to sapienza2023adm.s3.eu-south-1.amazonaws.com (sapienza2023adm.s3.eu-south-1.amazonaws.com)|52.95.153.12|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 16123393395 (15G) [application/json]\n",
            "Saving to: ‘/content/drive/MyDrive/Colab Notebooks/lighter_books.json’\n",
            "\n",
            "lighter_books.json  100%[===================>]  15.02G  29.5MB/s    in 9m 50s  \n",
            "\n",
            "2023-11-02 10:40:27 (26.1 MB/s) - ‘/content/drive/MyDrive/Colab Notebooks/lighter_books.json’ saved [16123393395/16123393395]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget -P \"/content/drive/MyDrive/Colab Notebooks\" https://sapienza2023adm.s3.eu-south-1.amazonaws.com/lighter_books.json\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -P \"/content/drive/MyDrive/Colab Notebooks\" https://sapienza2023adm.s3.eu-south-1.amazonaws.com/lighter_authors.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HLKDO5uqMgEu",
        "outputId": "82bf0b03-c8bd-4d73-e704-cf27551898fe"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-11-02 10:40:27--  https://sapienza2023adm.s3.eu-south-1.amazonaws.com/lighter_authors.json\n",
            "Resolving sapienza2023adm.s3.eu-south-1.amazonaws.com (sapienza2023adm.s3.eu-south-1.amazonaws.com)... 52.95.151.26, 52.95.151.42\n",
            "Connecting to sapienza2023adm.s3.eu-south-1.amazonaws.com (sapienza2023adm.s3.eu-south-1.amazonaws.com)|52.95.151.26|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 549013002 (524M) [application/json]\n",
            "Saving to: ‘/content/drive/MyDrive/Colab Notebooks/lighter_authors.json’\n",
            "\n",
            "lighter_authors.jso 100%[===================>] 523.58M  22.3MB/s    in 26s     \n",
            "\n",
            "2023-11-02 10:40:53 (20.4 MB/s) - ‘/content/drive/MyDrive/Colab Notebooks/lighter_authors.json’ saved [549013002/549013002]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filename = \"/content/drive/MyDrive/Colab Notebooks/lighter_authors.json\""
      ],
      "metadata": {
        "id": "GkssCsSgNEIW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filename1 = \"/content/drive/MyDrive/Colab Notebooks/lighter_books.json\""
      ],
      "metadata": {
        "id": "g9FGCfWeNOe6"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "XjcV_uMUNUHv"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_chunks = pd.read_json(filename, lines=True, chunksize=10000)\n",
        "# Create an empty DataFrame to store the results\n",
        "df = pd.DataFrame()\n",
        "\n",
        "# Loop through each chunk and append it to the result DataFrame\n",
        "for chunk in df_chunks:\n",
        "  df = pd.concat([df, chunk], ignore_index=True)"
      ],
      "metadata": {
        "id": "OzQ5MfXXNh02"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1_chunks = pd.read_json(filename1, lines=True, chunksize=10000)\n",
        "# Create an empty DataFrame to store the results\n",
        "df1 = pd.DataFrame()\n",
        "\n",
        "# Loop through each chunk and append it to the result DataFrame\n",
        "for chunk in df1_chunks:\n",
        "  chunk = chunk[['rating_dist']]\n",
        "  df1 = pd.concat([df1, chunk], ignore_index=True)"
      ],
      "metadata": {
        "id": "lL-ZWt5uPwo3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1_chunks = pd.read_json(filename1, lines=True, chunksize=10000)\n",
        "# Create an empty DataFrame to store the results\n",
        "df1 = pd.DataFrame()\n",
        "\n",
        "# Loop through each chunk and append it to the result DataFrame\n",
        "for chunk in df1_chunks:\n",
        "  chunk = chunk[['original_publication_date', 'author_id', 'publication_date']]\n",
        "  df1 = pd.concat([df1, chunk], ignore_index=True)"
      ],
      "metadata": {
        "id": "uLAF4yL9RPlP"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1_chunks = pd.read_json(filename1, lines=True, chunksize=10000)\n",
        "# Create an empty DataFrame to store the results\n",
        "df1 = pd.DataFrame()\n",
        "\n",
        "# Loop through each chunk and append it to the result DataFrame\n",
        "for chunk in df1_chunks:\n",
        "  chunk = chunk[['id', 'num_pages']]\n",
        "  df1 = pd.concat([df1, chunk], ignore_index=True)"
      ],
      "metadata": {
        "id": "-gqsEzJguDfA"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Estimate the probability that a book has over 30% of ratings above 4\n",
        "\n",
        "#Parse the string to extract the ratings distribution.\n",
        "def parse_ratings(s):\n",
        "   ratings = s.split(\"|\")\n",
        "   ratings_dict = {}\n",
        "   for rating in ratings:\n",
        "       if \"total\" in rating:\n",
        "           rating, count = rating.split(\":\")\n",
        "           ratings_dict[rating] = int(count)\n",
        "       else:\n",
        "           rating, count = rating.split(\":\")\n",
        "           ratings_dict[int(rating)] = int(count)\n",
        "\n",
        "   return ratings_dict\n",
        "\n",
        "#get total ratings and the percentage of ratings above 4\n",
        "def get_total_ratings(ratings_dict):\n",
        "   return ratings_dict['total']\n",
        "\n",
        "def get_percentage_above_4(ratings_dict, total_ratings):\n",
        "   above_4_sum = sum(count for rating, count in ratings_dict.items() if rating!='total' and rating > 4)\n",
        "   if total_ratings!=0:\n",
        "        return (above_4_sum / total_ratings)*100\n",
        "   else:\n",
        "        return (0)\n",
        "\n",
        "\n",
        "def check_probability(ratings_string):\n",
        "   ratings_dict = parse_ratings(ratings_string)\n",
        "   total_ratings = get_total_ratings(ratings_dict)\n",
        "   percentage_above_4 = get_percentage_above_4(ratings_dict, total_ratings)\n",
        "   return percentage_above_4 >= 30\n",
        "\n",
        "df1[\"above_30_percent_4\"] = df1[\"rating_dist\"].apply(check_probability)\n",
        "\n",
        "count=0\n",
        "for i in df1[\"above_30_percent_4\"]:\n",
        "  if i==True:\n",
        "    count+=1\n",
        "\n",
        "print((count/len(df1[\"above_30_percent_4\"]))*100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nA_bLrzZHp6T",
        "outputId": "7714e31d-6cd7-49ee-c961-d2070a4c910d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "45.015554048129395\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Estimate the probability that an author publishes a new book within two years from its last work.\n",
        "\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "#we want to take in consideration only the first edition of a book as \"new book\"\n",
        "df1_filtered = df1[df1['original_publication_date'] == df1['publication_date']]\n",
        "\n",
        "df1_grouped = df1_filtered.groupby('author_id')['original_publication_date']\n",
        "author_dict = df1_grouped.apply(list).to_dict()\n",
        "lessthan2=0\n",
        "morethan2=0\n",
        "for author_id, original_publication_date in author_dict.items():\n",
        "\n",
        "  #formatting and sorting the values in datetime\n",
        "  try:\n",
        "    original_publication_date = sorted([datetime.strptime(x, '%Y-%m-%d') for x in original_publication_date if x is not None])\n",
        "  except ValueError:\n",
        "    continue #values that cannot be converted will be eliminated\n",
        "\n",
        "  #calculating time differences\n",
        "  time_differences = []\n",
        "  for i in range(len(original_publication_date)-1):\n",
        "    try:\n",
        "      diff = original_publication_date[i+1]-original_publication_date[i]\n",
        "      diff = diff.total_seconds()/(3600*24*365)\n",
        "      if diff!=0: #we assume you can't publish multiple books in the same day\n",
        "        time_differences.append(diff)\n",
        "    except TypeError:\n",
        "      continue #if the difference can't be done it will not count\n",
        "\n",
        "  #counting the number of times the difference of publication is less or more than 2 years\n",
        "  if time_differences: #when it's not an empty list\n",
        "    for i in time_differences:\n",
        "      if i<2:\n",
        "        lessthan2+=1\n",
        "      else:\n",
        "        morethan2+=1\n",
        "  else:\n",
        "    continue\n",
        "\n",
        "print(lessthan2/(lessthan2+morethan2)) #the final result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ueNFimhxoEXo",
        "outputId": "feb31108-5aa1-4130-f39f-e1338ba1406d"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7721301078191538\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#In the file list.json, you will find a peculiar list named \"The Worst Books of All Time.\"\n",
        "#Estimate the probability of a book being included in this list, knowing it has more than 700 pages.\n",
        "#Are the events X=’Being Included in The Worst Books of All Time list’ and Y=’Having more than 700 pages’ independent?\n",
        "#Explain how you have obtained your answer.\n",
        "\n",
        "#creating a list with all the ids of the books in the list Worst Books of All Time (wboat)\n",
        "df3=pd.read_json(\"list.json\", nrows=10, lines=True) #we actually only need the first row\n",
        "book_ids_wboat=list()\n",
        "for i in range(len(df3['books'][0])): #df3['books'][0] is the list WBOAT and we don't need nothing else\n",
        "  book_ids_wboat.append(df3['books'][0][i]['book_id'])\n",
        "book_ids_wboat = [int(i) for i in book_ids_wboat] #converting to integers\n",
        "\n",
        "#formatting the 'number of pages' column and filtering the ones with more than 700 pages\n",
        "import re\n",
        "\n",
        "df1_modified=df1.dropna(subset=['num_pages'])\n",
        "df1_modified['num_pages'] = df1_modified['num_pages'].apply(lambda x: int(x) if (isinstance(x, (float, int)) or bool(re.search(r\"^\\d+$\",x))) else None)\n",
        "df1_filtered=df1_modified[df1_modified['num_pages']>700]\n",
        "\n",
        "#checking which books are in both the wboat-list and the filtered books dataframe\n",
        "def is_in_wboat(book_id):\n",
        "  return(book_id in book_ids_wboat)\n",
        "df1[\"wboat_700\"]=df1_filtered['id'].apply(is_in_wboat)\n",
        "\n",
        "#counting the number of books in the intersection and calculating the probability P(X|Y)\n",
        "count=0\n",
        "for i in df1[\"wboat_700\"]:\n",
        "  if i==True:\n",
        "    count+=1\n",
        "print(count/len(df1_filtered))\n",
        "\n",
        "#to answer the last question we count the number of books in the intersection between the wboat-list\n",
        "#and all the books regardless of the number of pages, then calculating P(X)\n",
        "df1[\"wboat\"]=df1_modified['id'].apply(is_in_wboat)\n",
        "count=0\n",
        "for i in df1[\"wboat\"]:\n",
        "  if i==True:\n",
        "    count+=1\n",
        "print(count/len(df1_modified))\n",
        "#the two numbers printed represent P(X|Y) and P(X) respectively,\n",
        "#since they are different it means that the events are NOT Indipendent\n"
      ],
      "metadata": {
        "id": "eh4Qkfz1mgXW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zjPfwjEQusYh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}